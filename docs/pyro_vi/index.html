<!doctype html><html lang=en dir=ltr><head><meta name=generator content="Hugo 0.79.1"><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="MathJax.Hub.Config({ tex2jax: { inlineMath: [['$', '$'] ], displayMath: [ ['$$','$$'], [&#34;\\[&#34;,&#34;\\]&#34;] ] } });  Pyroでの変分推論 #  前節で変分近似は、
 対象の事象の発生過程を確率モデルとしてモデリングする。 推定したいパラメータが従う事後確率分布の近似関数（変分関数）を仮定する。 変分関数と真の事後関数のKLダイバージェンスを最小化する（＝ELBOを最大化する）。  という手順で行うことを説明しました。この節ではPyroを用いて変分推論を具体的に行っていく手順を見ていきます。Pyroでも上の手順をたどっていくことになります。以下の簡単な例を用いてその手順を１つずつ見ていくことにします。
例：袋の中のボールの比率の推論
赤玉と白玉が入っている中身が見えない袋があります。袋の中の赤玉と白玉の数は同数入っている（混合比率=0.5）という事前情報がありますが、実際のところはわかっていません。そこで袋の中からランダムに１つを取り出しそのボールの色を確認後、箱の中に戻すという操作を複数回行います。 この時、10回の試行で赤が６回、白が４回が出た場合、玉の混合比率についてどういう推論が可能でしょうか？ベイズ推論の枠組みに従い混合比率の確率分布をPyroを用いて推論していくことにします。
※ 以下のコードの全体は Githubリポジトリに置いています。
試行データ #  ここでまず上記例の試行結果のデータを作っておきます。
# 試行データ作成 def create_data(red_num, white_num): red = torch.tensor(1.0) white = torch.tensor(0.0) data = [] for _ in range(red_num): data.append(red) for _ in range(white_num): data.append(white) random.shuffle(data) data = torch.tensor(data) return data data = create_data(6, 4) 確率モデルの構築 #  混合比率の推論に向けて、まず最初に今回の事象が発生する確率モデルを構築します。"><meta name=theme-color content="#FFFFFF"><meta property="og:title" content="Pyroでの変分推論"><meta property="og:description" content><meta property="og:type" content="website"><meta property="og:url" content="https://pyro-book.data-hacker.net/docs/pyro_vi/"><title>Pyroでの変分推論 | Pyroで実践するベイズ機械学習</title><link rel=manifest href=https://pyro-book.data-hacker.net/manifest.json><link rel=icon href=https://pyro-book.data-hacker.net/favicon.png type=image/x-icon><link rel=stylesheet href=https://pyro-book.data-hacker.net/book.min.6c7c6446dfdee7c8c933e9bbc6e80ee3ed6c913b2a59519f2092c3c6a9d63e55.css integrity="sha256-bHxkRt/e58jJM+m7xugO4+1skTsqWVGfIJLDxqnWPlU="><script defer src=https://pyro-book.data-hacker.net/en.search.min.d2ad2dfd45d1981c389816c0ad6753f20b52850c3378cdb0637c3374f17e3822.js integrity="sha256-0q0t/UXRmBw4mBbArWdT8gtShQwzeM2wY3wzdPF+OCI="></script><script defer src=https://pyro-book.data-hacker.net/sw.min.74a8bb07f0bee86d6bb9a2750f073f14d93c7e4512f28860370cfd879e9719b4.js integrity="sha256-dKi7B/C+6G1ruaJ1Dwc/FNk8fkUS8ohgNwz9h56XGbQ="></script><link rel=alternate type=application/rss+xml href=https://pyro-book.data-hacker.net/docs/pyro_vi/index.xml title=Pyroで実践するベイズ機械学習></head><body dir=ltr><input type=checkbox class="hidden toggle" id=menu-control>
<input type=checkbox class="hidden toggle" id=toc-control><main class="container flex"><aside class=book-menu><div class=book-menu-content><nav><h2 class=book-brand><a href=https://pyro-book.data-hacker.net/><span>Pyroで実践するベイズ機械学習</span></a></h2><div class=book-search><input type=text id=book-search-input placeholder=Search aria-label=Search maxlength=64 data-hotkeys=s/><div class="book-search-spinner hidden"></div><ul id=book-search-results></ul></div><ul><li><a href=https://pyro-book.data-hacker.net/docs/what_is_pyro/>Pyroとは</a></li><li><a href=https://pyro-book.data-hacker.net/docs/pyro_modeling/>Pyroによる確率モデリング</a></li><li><a href=https://pyro-book.data-hacker.net/docs/dist_basic/>確率分布の取り扱い</a></li><li><a href=https://pyro-book.data-hacker.net/docs/bayes_learning_basic/>ベイズ学習の枠組み</a></li><li><a href=https://pyro-book.data-hacker.net/docs/vi_basic/>変分推論の基礎</a></li><li><a href=https://pyro-book.data-hacker.net/docs/pyro_vi/ class=active>Pyroでの変分推論</a></li><li><a href=https://pyro-book.data-hacker.net/docs/mle_map/>MAP推定と最尤推定</a></li><li><a href=https://pyro-book.data-hacker.net/docs/linear_regression/>ベイズ線形回帰</a></li><li><a href=https://pyro-book.data-hacker.net/docs/model_selection_01/>モデル選択:周辺尤度最大化</a><br><br></li></ul><ul><li><a href=https://github.com/a-mitani/bayes-pyro target=_blank rel=noopener>Github</a></li></ul></nav><script>(function(){var menu=document.querySelector("aside.book-menu nav");addEventListener("beforeunload",function(event){localStorage.setItem("menu.scrollTop",menu.scrollTop);});menu.scrollTop=localStorage.getItem("menu.scrollTop");})();</script></div></aside><div class=book-page><header class=book-header><div class="flex align-center justify-between"><label for=menu-control><img src=https://pyro-book.data-hacker.net/svg/menu.svg class=book-icon alt=Menu></label>
<strong>Pyroでの変分推論</strong>
<label for=toc-control><img src=https://pyro-book.data-hacker.net/svg/toc.svg class=book-icon alt="Table of Contents"></label></div><aside class="hidden clearfix"><nav id=TableOfContents><ul><li><a href=#pyroでの変分推論>Pyroでの変分推論</a><ul><li><a href=#試行データ>試行データ</a></li><li><a href=#確率モデルの構築>確率モデルの構築</a></li><li><a href=#変分関数を仮定>変分関数を仮定</a></li><li><a href=#最適化>最適化</a></li></ul></li></ul></nav></aside></header><article class=markdown><script type=text/javascript async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML"></script><script type=text/x-mathjax-config>
 MathJax.Hub.Config({
 tex2jax: {
 inlineMath: [['$', '$'] ],
 displayMath: [ ['$$','$$'], ["\\[","\\]"] ]
 }
 });
</script><h1 id=pyroでの変分推論>Pyroでの変分推論
<a class=anchor href=#pyro%e3%81%a7%e3%81%ae%e5%a4%89%e5%88%86%e6%8e%a8%e8%ab%96>#</a></h1><p>前節で変分近似は、</p><ol><li>対象の事象の発生過程を確率モデルとしてモデリングする。</li><li>推定したいパラメータが従う事後確率分布の近似関数（変分関数）を仮定する。</li><li>変分関数と真の事後関数のKLダイバージェンスを最小化する（＝ELBOを最大化する）。</li></ol><p>という手順で行うことを説明しました。この節ではPyroを用いて変分推論を具体的に行っていく手順を見ていきます。Pyroでも上の手順をたどっていくことになります。以下の簡単な例を用いてその手順を１つずつ見ていくことにします。</p><p><strong>例：袋の中のボールの比率の推論</strong></p><p>赤玉と白玉が入っている中身が見えない袋があります。袋の中の赤玉と白玉の数は同数入っている（混合比率=0.5）という事前情報がありますが、実際のところはわかっていません。そこで袋の中からランダムに１つを取り出しそのボールの色を確認後、箱の中に戻すという操作を複数回行います。
この時、10回の試行で赤が６回、白が４回が出た場合、玉の混合比率についてどういう推論が可能でしょうか？ベイズ推論の枠組みに従い混合比率の確率分布をPyroを用いて推論していくことにします。</p><p>※ 以下のコードの全体は
<a href=https://github.com/a-mitani/pyro_code_examples/blob/main/svi-basic.ipynb>Githubリポジトリ</a>に置いています。</p><h2 id=試行データ>試行データ
<a class=anchor href=#%e8%a9%a6%e8%a1%8c%e3%83%87%e3%83%bc%e3%82%bf>#</a></h2><p>ここでまず上記例の試行結果のデータを作っておきます。</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=color:#75715e># 試行データ作成</span>
<span style=color:#66d9ef>def</span> <span style=color:#a6e22e>create_data</span>(red_num, white_num):
    red <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>tensor(<span style=color:#ae81ff>1.0</span>)
    white <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>tensor(<span style=color:#ae81ff>0.0</span>)
    data <span style=color:#f92672>=</span> []
    <span style=color:#66d9ef>for</span> _ <span style=color:#f92672>in</span> range(red_num):
        data<span style=color:#f92672>.</span>append(red)
    <span style=color:#66d9ef>for</span> _ <span style=color:#f92672>in</span> range(white_num):
        data<span style=color:#f92672>.</span>append(white)
    random<span style=color:#f92672>.</span>shuffle(data)
    data <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>tensor(data)
    <span style=color:#66d9ef>return</span> data

data <span style=color:#f92672>=</span> create_data(<span style=color:#ae81ff>6</span>, <span style=color:#ae81ff>4</span>)
</code></pre></div><h2 id=確率モデルの構築>確率モデルの構築
<a class=anchor href=#%e7%a2%ba%e7%8e%87%e3%83%a2%e3%83%87%e3%83%ab%e3%81%ae%e6%a7%8b%e7%af%89>#</a></h2><p>混合比率の推論に向けて、まず最初に今回の事象が発生する確率モデルを構築します。</p><p>$i$回目の試行で取り出される玉の色を表す確率変数を$X_i \in \lbrace\text{赤, 白}\rbrace$とします。例えば混合率（今回は赤玉の比率）を$\theta$とすると、$\theta$に応じて$X_i$の実現値が決まると考えられるでしょう。ベイズ機械学習ではこの混合比率$\theta$も確率的に決まる値であり確率変数$\Theta$の実現値と考えます。つまり取り出される玉の色は以下の過程で決まるとモデル化します。<sup id=fnref:1><a href=#fn:1 class=footnote-ref role=doc-noteref>1</a></sup></p><ol><li>混合率の実現値$\theta$が、事前確率分布$p(\Theta)$に従って決まる。</li><li>混合率が$\theta$と決まった条件下で、$\theta$に応じて確率$p(X_i|\Theta=\theta)$で玉の色が決まる。</li></ol><p>この確率モデルをグラフィカルモデルで記述すると以下の図のようになります。今回の試行は混合率が$\Theta=\theta$と決まった条件下で各試行間は独立の関係（条件付き独立）となっています。そこで「プレート表現」を使って$N$回の試行を１つにまとめて表現しています。また玉の色は観測されるもので事後確率を考える上での「条件」となるので変数Xに相当するノードが塗り潰されています<sup id=fnref:2><a href=#fn:2 class=footnote-ref role=doc-noteref>2</a></sup>。</p><center><img src=ball_model.png width=300></center><p>またこの時、試行回数$N$とした時の同時確率分布は以下のように書けます。ここで$x_i$は$i$回目での思考の玉の色の実現値を示しています。
$$p(\mathbf{X}=\mathbf{x}, \Theta=\theta)=\prod_{i=1}^{N} p(X_i=x_i|\Theta=\theta)~p(\Theta=\theta)$$</p><p>さて、上記ステップ2.の取り出される色の確率分布は比率$\theta$をパラメータとしたベルヌーイ分布と仮定するのが自然でしょう。つまり
$$p(X_i|\Theta=\theta)=\operatorname{Bern}(X_i, \theta)$$</p><p>と仮定します。
また今回の例の場合、袋の中には赤玉と白玉同数入っているという事前情報があるのでそれをモデルに取り込むために、比率0.5に穏やかなピークをもつベータ分布
$$p(\Theta)=\operatorname{Beta}(\Theta|\alpha = 2, \beta = 2)$$
と仮定します<sup id=fnref:3><a href=#fn:3 class=footnote-ref role=doc-noteref>3</a></sup>。</p><p>この確率モデルをPyroで記述してみます。前節で述べたとおりPyroでは確率モデルを確率プリミティブを組み合わせた関数の形で記述します。今回のモデルは以下の様に実装することができます。</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=color:#75715e># 確率モデルの定義</span>
<span style=color:#66d9ef>def</span> <span style=color:#a6e22e>model</span>(data):
    <span style=color:#75715e># 事前確率分布は比率0.5に穏やかなピークを持つ関数を仮定する。</span>
    alpha0 <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>tensor(<span style=color:#ae81ff>2.0</span>)
    beta0 <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>tensor(<span style=color:#ae81ff>2.0</span>)
    f <span style=color:#f92672>=</span> pyro<span style=color:#f92672>.</span>sample(<span style=color:#e6db74>&#34;Theta&#34;</span>, dist<span style=color:#f92672>.</span>Beta(alpha0, beta0))

    <span style=color:#75715e># 観測データのプレート定義</span>
    <span style=color:#66d9ef>with</span> pyro<span style=color:#f92672>.</span>plate(<span style=color:#e6db74>&#39;observation&#39;</span>):
      pyro<span style=color:#f92672>.</span>sample(<span style=color:#e6db74>&#39;X&#39;</span>, dist<span style=color:#f92672>.</span>Bernoulli(f), obs<span style=color:#f92672>=</span>data)
</code></pre></div><p>Pyroでは条件付き独立な関係の確率変数を,グラフィカルモデルと同様に<code>plate</code>としてまとめる機能を持っており、8,9行目ではそれを利用してモデル化しているのに注意してください。</p><h2 id=変分関数を仮定>変分関数を仮定
<a class=anchor href=#%e5%a4%89%e5%88%86%e9%96%a2%e6%95%b0%e3%82%92%e4%bb%ae%e5%ae%9a>#</a></h2><p>今回、我々は玉を取り出した結果をもとに玉の混合比率の確率分布$p(\Theta|X)$を求めようとしています。変分近似ではこの未知の分布をなんらかパラメータを用いて簡単に記述できる分布（変分関数）を仮定してそのパラメータを推定するのでした。ここでは変分関数としてベルヌーイ分布の共役事前分布であるベータ分布を採用し、ベータ分布のパラメータ$\alpha, \beta$を推定する問題に帰着させます。</p><p>Pyroでは変分関数の定義もモデルの定義と同様に確率プリミティブを組み合わせた関数の形で記述します。Pyroではこの変分関数を定義する関数を<code>guide</code>と呼びます。</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>guide</span>(data):
    <span style=color:#75715e># 変分パラメータαとβを定義する。</span>
    <span style=color:#75715e># 初期値は共に10としている。</span>
    <span style=color:#75715e># また、ベータ分布においてこれらのパラメータは正の値なので`constraints.positive`を指定。</span>
    alpha_q <span style=color:#f92672>=</span> pyro<span style=color:#f92672>.</span>param(<span style=color:#e6db74>&#34;alpha_q&#34;</span>, torch<span style=color:#f92672>.</span>tensor(<span style=color:#ae81ff>10.0</span>),
                         constraint<span style=color:#f92672>=</span>constraints<span style=color:#f92672>.</span>positive)
    beta_q <span style=color:#f92672>=</span> pyro<span style=color:#f92672>.</span>param(<span style=color:#e6db74>&#34;beta_q&#34;</span>, torch<span style=color:#f92672>.</span>tensor(<span style=color:#ae81ff>10.0</span>),
                        constraint<span style=color:#f92672>=</span>constraints<span style=color:#f92672>.</span>positive)
    <span style=color:#75715e># 最適化されたパラメータのベータ分布から混合率Θをサンプリングする</span>
    pyro<span style=color:#f92672>.</span>sample(<span style=color:#e6db74>&#34;Theta&#34;</span>, dist<span style=color:#f92672>.</span>Beta(alpha_q, beta_q))
</code></pre></div><p><code>guide</code>を定義する際、以下の点に注意する必要があります。</p><ul><li><code>guide</code>関数の引数は<code>model</code>関数を定義した際の引数と同一であること。</li><li>学習対象である変分パラメータ（ここでは$\alpha, \beta$）を<code>pyro.param</code>を用いて定義します。これは後にELBOの各パラメータの偏微分値を求めるために、<code>requires_grad</code>フラグを<code>True</code>にセットした<code>torch.tensor</code>型の変数として各変分パラメータを定義していることに相当します。</li></ul><p>またここではベータ分布において$\alpha, \beta$は正の値のため<code>constraint=constraints.positive</code>として最適化過程で取りえる値に制約を加えています。</p><h2 id=最適化>最適化
<a class=anchor href=#%e6%9c%80%e9%81%a9%e5%8c%96>#</a></h2><p>変分関数が定義できたので、その中で変分パラメータとして定義した<code>alpha_q</code>と<code>alpha_q</code>をELBOが最大化するように最適化計算を行います。具体的には以下のコードのように実装します。</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=color:#75715e># グローバル変数として保存されているパラメータを削除</span>
pyro<span style=color:#f92672>.</span>clear_param_store()

<span style=color:#75715e># Optimizerの定義と設定（Adamの利用が推奨されている）</span>
adam_params <span style=color:#f92672>=</span> {<span style=color:#e6db74>&#34;lr&#34;</span>: <span style=color:#ae81ff>0.002</span>, <span style=color:#e6db74>&#34;betas&#34;</span>: (<span style=color:#ae81ff>0.95</span>, <span style=color:#ae81ff>0.999</span>)}
optimizer <span style=color:#f92672>=</span> Adam(adam_params)

<span style=color:#75715e># 推論アルゴリズムとLoss値を定義</span>
<span style=color:#75715e># ここでは組み込みのELBOの符号反転をLoss値とする`Trace_ELBO()`を利用しています。</span>
svi <span style=color:#f92672>=</span> SVI(model, guide, optimizer, loss<span style=color:#f92672>=</span>Trace_ELBO())

<span style=color:#75715e># 最適化の逐次計算</span>
<span style=color:#75715e># ここではAdamで勾配降下を1000回繰り返すことになる。</span>
n_steps <span style=color:#f92672>=</span> <span style=color:#ae81ff>1000</span>
losses <span style=color:#f92672>=</span> []
<span style=color:#66d9ef>for</span> step <span style=color:#f92672>in</span> range(n_steps):
    loss <span style=color:#f92672>=</span> svi<span style=color:#f92672>.</span>step(data)
    losses<span style=color:#f92672>.</span>append(loss)
    <span style=color:#66d9ef>if</span> step <span style=color:#f92672>%</span> <span style=color:#ae81ff>100</span> <span style=color:#f92672>==</span> <span style=color:#ae81ff>0</span>:
        <span style=color:#66d9ef>print</span>(<span style=color:#e6db74>&#39;#&#39;</span>, end<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;&#39;</span>)

plt<span style=color:#f92672>.</span>plot(losses)
</code></pre></div><p>ここで、最適化を行う前に、<code>pyro.clear_param_store()</code>を実行しています。これはPyroではグローバル変数として各変分パラメータを保持しているため、それを消す処理を行っています。
またこのコードでは、<code>svi.step()</code>はLoss値（ELBOの符号逆転値）が返却されるので、Lossの変化をプロットしています。結果は以下の図のようになります。SVI(Stochastic Variational Inference)は確率的勾配降下法(SGD)と同様に計算速度を上げるためにミニバッチで勾配計算を行う<sup id=fnref:4><a href=#fn:4 class=footnote-ref role=doc-noteref>4</a></sup>ためLossがランダムに振動していますが、1000回イタレーションを繰り返すと、ELBOが小さいところで落ち着いているのが見てとれます。</p><center><img src=svi_ball_elbo.png width=300></center><p>この最適化後の変分パラメータは以下のコードのように<code>pyro.param</code>で取得できます。</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=color:#75715e># 最適化後の変分パラメータを取得する</span>
alpha_q <span style=color:#f92672>=</span> pyro<span style=color:#f92672>.</span>param(<span style=color:#e6db74>&#34;alpha_q&#34;</span>)<span style=color:#f92672>.</span>item()
beta_q <span style=color:#f92672>=</span> pyro<span style=color:#f92672>.</span>param(<span style=color:#e6db74>&#34;beta_q&#34;</span>)<span style=color:#f92672>.</span>item()
<span style=color:#66d9ef>print</span>(<span style=color:#e6db74>&#34;alpha_q = {:.2f}, beta_q = {:.2f}&#34;</span><span style=color:#f92672>.</span>format(alpha_q, beta_q))
<span style=color:#75715e>## Output</span>
<span style=color:#75715e>#alpha_q = 10.55, beta_q = 7.80</span>
</code></pre></div><p>我々は混合分布の事後確率分布をベータ分布と仮定していましたから、上記のパラメータを用いて事後確率分布の様子をプロットして確認してみましょう。</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=color:#75715e># 得られたパラメータを用いて事後確率分布をプロット</span>
x_range <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>arange(<span style=color:#ae81ff>0.0</span>, <span style=color:#ae81ff>1.0</span>, <span style=color:#ae81ff>0.01</span>)
estimated_dist <span style=color:#f92672>=</span> dist<span style=color:#f92672>.</span>Beta(alpha_q, beta_q)
y <span style=color:#f92672>=</span> [estimated_dist<span style=color:#f92672>.</span>log_prob(torch<span style=color:#f92672>.</span>tensor([x]))<span style=color:#f92672>.</span>exp() <span style=color:#66d9ef>for</span> x <span style=color:#f92672>in</span> x_range]
plt<span style=color:#f92672>.</span>plot(x_range, y)
</code></pre></div><p>得られるグラフは下図のようになります。事後分布$p(\Theta|X)$は0.6付近が最も大きくなっており、これは10回の試行で赤が6回出た観測事象と辻褄が合っているのがわかります。ただし10回の観測ではまだ混合比率に曖昧性があり、確率分布の裾野が比較的広い確率分布となります。</p><center><img src=pdf_beta_10.png width=300></center><p>より具体的に今回推論された確率分布の最頻値を求めてみましょう。
計算結果は以下のようになります。
ここで、最頻値0.584となっており、観測結果の0.6より小さくなっているのに注意してください。これは事前確率として0.5をピークに持つ関数を設定していることにより起きています。観測だけを信じると0.6ですが、試行回数が10回程度であればその結果だけを信じて取り込むよりも事前情報の0.5もある程度加味された事後分布になっているという状況です。</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=color:#75715e># 最頻値を計算</span>
mode <span style=color:#f92672>=</span> (alpha_q <span style=color:#f92672>-</span> <span style=color:#ae81ff>1</span>) <span style=color:#f92672>/</span> (alpha_q <span style=color:#f92672>+</span> beta_q <span style=color:#f92672>-</span> <span style=color:#ae81ff>2</span>)
<span style=color:#66d9ef>print</span>(<span style=color:#e6db74>&#34;mode = {:.3f}&#34;</span><span style=color:#f92672>.</span>format(mode))

<span style=color:#75715e># 平均値を計算</span>
mean <span style=color:#f92672>=</span> alpha_q <span style=color:#f92672>/</span> (alpha_q <span style=color:#f92672>+</span> beta_q)
<span style=color:#75715e># 標準偏差を計算</span>
factor <span style=color:#f92672>=</span> beta_q <span style=color:#f92672>/</span> (alpha_q <span style=color:#f92672>*</span> (<span style=color:#ae81ff>1.0</span> <span style=color:#f92672>+</span> alpha_q <span style=color:#f92672>+</span> beta_q))
std <span style=color:#f92672>=</span> mean <span style=color:#f92672>*</span> np<span style=color:#f92672>.</span>sqrt(factor)
<span style=color:#66d9ef>print</span>(<span style=color:#e6db74>&#34;infered ratio = {:.3f} +- {:.3f}&#34;</span><span style=color:#f92672>.</span>format(mean, std))
<span style=color:#75715e>## Output</span>
<span style=color:#75715e># mode = 0.584</span>
<span style=color:#75715e># infered ratio = 0.575 +- 0.112</span>
</code></pre></div><p>さらに観測回数を増やして1000回の試行でそのうち赤が600回、白が400回取り出された場合はどのような混合比率の事後分布になるでしょうか？
試行データ作成時に<code>data = create_data(600, 400)</code>としてデータを作成して同様の推論を行ってみます。
すると結果の事後分布は下記のような形となり、10回の時よりも混合比率が0.6である確信が強まり、確率分布の裾野が10回の思考の場合よりも狭まっているのがわかります。</p><center><img src=pdf_beta_100.png width=300></center><p>以上、Pyroを用いて簡単な例での変分推定のやり方を見てきました。今回のコードは
<a href=https://github.com/a-mitani/pyro_code_examples/blob/main/svi-basic.ipynb>Github</a>に配置していますので参考にしてみてください。</p><section class=footnotes role=doc-endnotes><hr><ol><li id=fn:1 role=doc-endnote><p>この様に観測事象の生成過程を仮定してモデリングする確率過程を<strong>生成モデル</strong>と呼びます。 <a href=#fnref:1 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:2 role=doc-endnote><p>グラフィカルモデルの詳細は例えば
<a href=https://projecteuclid.org/journals/statistical-science/volume-19/issue-1/Graphical-Models/10.1214/088342304000000026.full>このレビュー論文</a>でわかりやすく解説されているので参照してください。 <a href=#fnref:2 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:3 role=doc-endnote><p>ここで事前分布としてベータ関数を採用のは単に確率変数の定義域が0~1かつ一様分布の確率分布を採用したいという動機であり、後のステップで変分関数としてベータ関数を採用するのとは無関係であることに注意してください。 <a href=#fnref:3 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:4 role=doc-endnote><p><a href=https://arxiv.org/abs/1601.00670>Variational Inference: A Review for Statisticians</a> <a href=#fnref:4 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li></ol></section></article><footer class=book-footer><div class="flex flex-wrap justify-between"></div></footer><div class=book-comments></div><label for=menu-control class="hidden book-menu-overlay"></label></div><aside class=book-toc><div class=book-toc-content><nav id=TableOfContents><ul><li><a href=#pyroでの変分推論>Pyroでの変分推論</a><ul><li><a href=#試行データ>試行データ</a></li><li><a href=#確率モデルの構築>確率モデルの構築</a></li><li><a href=#変分関数を仮定>変分関数を仮定</a></li><li><a href=#最適化>最適化</a></li></ul></li></ul></nav></div></aside></main></body></html>
<!doctype html><html lang=en dir=ltr><head><meta name=generator content="Hugo 0.79.1"><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="MathJax.Hub.Config({ tex2jax: { inlineMath: [['$', '$'] ], displayMath: [ ['$$','$$'], [&#34;\\[&#34;,&#34;\\]&#34;] ] } });  変分推論の基礎 #  前節でベイズ学習の枠組みの３つの要点を簡単に説明しました。つまり
  ベイズ学習とは観測データ$\mathbf{X}$が得られたという条件下での未知のパラメータ$\mathbf{w}$の確率分布、すなわち$p(\mathbf{w}|\mathbf{X})$を学習する作業であること。
  一般的にベイズ学習は①対象とする事象の発生過程のモデル化（確率モデルの構築）を行ったうえで、②確率モデルと観測データをもとに未知パラメータを推論する、という枠組みで行うこと。
  条件付き確率の定義から、$W$が離散変数の場合、$$p(\mathbf{w}|\mathbf{X})=\frac{p(\mathbf{w},\mathbf{X})}{p(\mathbf{X})}=\frac{p(\mathbf{w},\mathbf{X})}{\sum_{\mathbf{w}} p(\mathbf{w},\mathbf{X})}\tag{1}$$もしくは$W$が連続変数の場合、$$p(\mathbf{w}|\mathbf{X})=\frac{p(\mathbf{w},\mathbf{X})}{p(\mathbf{X})}=\frac{p(\mathbf{w},\mathbf{X})}{\int_{\mathbf{w}} p(\mathbf{w},\mathbf{X})d\mathbf{w}}\tag{2}$$を計算することで$p(\mathbf{w}|\mathbf{X})$を推論することができること。
  の３点です。
しかし、前節で例示したような簡単な例を除いて、現実のほとんどの問題では、(1)式や(2)式の周辺分布（つまり和や積分の部分）は計算量が膨大であったり積分が解析的に不可能であり、厳密に計算できません。そこでこの周辺分布の計算を近似的にかつ現実的な時間内で計算する手法としてサンプリングや変分近似の手法が考案されてきました。ここではその１つである変分近似の手法について解説します。
※ 以降では確率変数が連続変数の場合に限って説明をしていきますが離散変数の場合でも同様の議論が可能です。
変分近似 #  我々は$p(\mathbf{w}|\mathbf{X})$を求めたいわけですが、この未知の確率分布がなんらかシンプルな関数$q(\mathbf{w})$で表現できないかと考えます。この$q(\mathbf{w})$を本来求めたい確率分布$p(\mathbf{w}|\mathbf{X})$に近づけていくことで$p(\mathbf{w}|\mathbf{X})$を近似的に求めてやろうという方法が変分近似です。
ここで$p(\mathbf{w}|\mathbf{X})$と近似関数$q(\mathbf{w})$の類似度合いの指標としてKLダイバージェンスを採用すると変分近似は、 $$ q_{opt.}(\mathbf{w})=\underset{q}{\operatorname{argmax}} \operatorname{KL}(q(\mathbf{w})||p(\mathbf{w}|\mathbf{X}))$$ の最適化問題として定式化できます。
しかしKLダイバージェンスに未知の関数である$p(\mathbf{w}|\mathbf{X})$が入っているため、このままでは$q_{opt.}$を求めることはできません。そこでこのKLダイバージェンスの最小化問題を、数学的なトリックを使って別の計算可能な量の最大化問題に書き換えることで間接的にKLダイバージェンスを最小化する関数を求めることを行います。ではどのようにするのでしょうか？
ELBO #  ここで対数周辺尤度$\ln{p(\mathbf{X})}$を以下のように書き換えることができることに着目します。 $$ \begin{align} \ln{p(\mathbf{X})} & = \ln{p(\mathbf{X})} \int_{\mathbf{w}} q(\mathbf{w}) d\mathbf{w}\newline & = \int_{\mathbf{w}} q(\mathbf{w}) \ln\frac{p(\mathbf{X}, \mathbf{w})}{p(\mathbf{w} \vert \mathbf{X})} d\mathbf{w}\newline & = \int_{\mathbf{w}} q(\mathbf{w}) \ln \frac{p(\mathbf{X}, \mathbf{w})~q(\mathbf{w})}{p(\mathbf{w} \vert \mathbf{X}) ~q(\mathbf{w})} d\mathbf{w}\newline & = \int_{\mathbf{w}} q(\mathbf{w}) \ln \frac{p(\mathbf{X}, \mathbf{w})}{q(\mathbf{w})} d\mathbf{w} + \int_{\mathbf{w}} q(\mathbf{w}) \ln \frac{q(\mathbf{w})}{p(\mathbf{w} \vert \mathbf{X})} d\mathbf{w}\newline & = \mathcal{L}(\mathbf{X}) + \operatorname{KL}(q\vert \vert p) \end{align} $$ ここで１行目は $$ \int_{\mathbf{w}} q(\mathbf{w}) d\mathbf{w} = 1 $$ を使っています。"><meta name=theme-color content="#FFFFFF"><meta property="og:title" content="変分推論の基礎"><meta property="og:description" content><meta property="og:type" content="website"><meta property="og:url" content="https://pyro-book.data-hacker.net/docs/vi_basic/"><title>変分推論の基礎 | Pyroで実践するベイズ機械学習</title><link rel=manifest href=https://pyro-book.data-hacker.net/manifest.json><link rel=icon href=https://pyro-book.data-hacker.net/favicon.png type=image/x-icon><link rel=stylesheet href=https://pyro-book.data-hacker.net/book.min.6c7c6446dfdee7c8c933e9bbc6e80ee3ed6c913b2a59519f2092c3c6a9d63e55.css integrity="sha256-bHxkRt/e58jJM+m7xugO4+1skTsqWVGfIJLDxqnWPlU="><script defer src=https://pyro-book.data-hacker.net/en.search.min.de1ea69d4abc533975cc39f6b8e509cf0c1e6337b780ac8fb6c518134dd329e7.js integrity="sha256-3h6mnUq8Uzl1zDn2uOUJzwweYze3gKyPtsUYE03TKec="></script><script defer src=https://pyro-book.data-hacker.net/sw.min.74a8bb07f0bee86d6bb9a2750f073f14d93c7e4512f28860370cfd879e9719b4.js integrity="sha256-dKi7B/C+6G1ruaJ1Dwc/FNk8fkUS8ohgNwz9h56XGbQ="></script><link rel=alternate type=application/rss+xml href=https://pyro-book.data-hacker.net/docs/vi_basic/index.xml title=Pyroで実践するベイズ機械学習></head><body dir=ltr><input type=checkbox class="hidden toggle" id=menu-control>
<input type=checkbox class="hidden toggle" id=toc-control><main class="container flex"><aside class=book-menu><div class=book-menu-content><nav><h2 class=book-brand><a href=https://pyro-book.data-hacker.net/><span>Pyroで実践するベイズ機械学習</span></a></h2><div class=book-search><input type=text id=book-search-input placeholder=Search aria-label=Search maxlength=64 data-hotkeys=s/><div class="book-search-spinner hidden"></div><ul id=book-search-results></ul></div><ul><li><a href=https://pyro-book.data-hacker.net/docs/what_is_pyro/>Pyroとは</a></li><li><a href=https://pyro-book.data-hacker.net/docs/pyro_modeling/>Pyroによる確率モデリング</a></li><li><a href=https://pyro-book.data-hacker.net/docs/dist_basic/>確率分布の取り扱い</a></li><li><a href=https://pyro-book.data-hacker.net/docs/bayes_learning_basic/>ベイズ学習の枠組み</a></li><li><a href=https://pyro-book.data-hacker.net/docs/vi_basic/ class=active>変分推論の基礎</a></li><li><a href=https://pyro-book.data-hacker.net/docs/pyro_vi/>Pyroでの変分推論</a></li><li><a href=https://pyro-book.data-hacker.net/docs/mle_map/>MAP推定と最尤推定</a></li><li><a href=https://pyro-book.data-hacker.net/docs/linear_regression/>ベイズ線形回帰</a><br><br></li></ul><ul><li><a href=https://github.com/a-mitani/bayes-pyro target=_blank rel=noopener>Github</a></li></ul></nav><script>(function(){var menu=document.querySelector("aside.book-menu nav");addEventListener("beforeunload",function(event){localStorage.setItem("menu.scrollTop",menu.scrollTop);});menu.scrollTop=localStorage.getItem("menu.scrollTop");})();</script></div></aside><div class=book-page><header class=book-header><div class="flex align-center justify-between"><label for=menu-control><img src=https://pyro-book.data-hacker.net/svg/menu.svg class=book-icon alt=Menu></label>
<strong>変分推論の基礎</strong>
<label for=toc-control><img src=https://pyro-book.data-hacker.net/svg/toc.svg class=book-icon alt="Table of Contents"></label></div><aside class="hidden clearfix"><nav id=TableOfContents><ul><li><a href=#変分推論の基礎>変分推論の基礎</a><ul><li><a href=#変分近似>変分近似</a></li><li><a href=#elbo>ELBO</a></li></ul></li></ul></nav></aside></header><article class=markdown><script type=text/javascript async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML"></script><script type=text/x-mathjax-config>
 MathJax.Hub.Config({
 tex2jax: {
 inlineMath: [['$', '$'] ],
 displayMath: [ ['$$','$$'], ["\\[","\\]"] ]
 }
 });
</script><h1 id=変分推論の基礎>変分推論の基礎
<a class=anchor href=#%e5%a4%89%e5%88%86%e6%8e%a8%e8%ab%96%e3%81%ae%e5%9f%ba%e7%a4%8e>#</a></h1><p>前節でベイズ学習の枠組みの３つの要点を簡単に説明しました。つまり</p><ol><li><p>ベイズ学習とは観測データ$\mathbf{X}$が得られたという条件下での未知のパラメータ$\mathbf{w}$の確率分布、すなわち$p(\mathbf{w}|\mathbf{X})$を学習する作業であること。</p></li><li><p>一般的にベイズ学習は①対象とする事象の発生過程のモデル化（確率モデルの構築）を行ったうえで、②確率モデルと観測データをもとに未知パラメータを推論する、という枠組みで行うこと。</p></li><li><p>条件付き確率の定義から、$W$が離散変数の場合、$$p(\mathbf{w}|\mathbf{X})=\frac{p(\mathbf{w},\mathbf{X})}{p(\mathbf{X})}=\frac{p(\mathbf{w},\mathbf{X})}{\sum_{\mathbf{w}} p(\mathbf{w},\mathbf{X})}\tag{1}$$もしくは$W$が連続変数の場合、$$p(\mathbf{w}|\mathbf{X})=\frac{p(\mathbf{w},\mathbf{X})}{p(\mathbf{X})}=\frac{p(\mathbf{w},\mathbf{X})}{\int_{\mathbf{w}} p(\mathbf{w},\mathbf{X})d\mathbf{w}}\tag{2}$$を計算することで$p(\mathbf{w}|\mathbf{X})$を推論することができること。</p></li></ol><p>の３点です。</p><p>しかし、前節で例示したような簡単な例を除いて、現実のほとんどの問題では、(1)式や(2)式の周辺分布（つまり和や積分の部分）は計算量が膨大であったり積分が解析的に不可能であり、厳密に計算できません。そこでこの周辺分布の計算を近似的にかつ現実的な時間内で計算する手法としてサンプリングや変分近似の手法が考案されてきました。ここではその１つである変分近似の手法について解説します。</p><p>※ 以降では確率変数が連続変数の場合に限って説明をしていきますが離散変数の場合でも同様の議論が可能です。</p><h2 id=変分近似>変分近似
<a class=anchor href=#%e5%a4%89%e5%88%86%e8%bf%91%e4%bc%bc>#</a></h2><p>我々は$p(\mathbf{w}|\mathbf{X})$を求めたいわけですが、この未知の確率分布がなんらかシンプルな関数$q(\mathbf{w})$で表現できないかと考えます。この$q(\mathbf{w})$を本来求めたい確率分布$p(\mathbf{w}|\mathbf{X})$に近づけていくことで$p(\mathbf{w}|\mathbf{X})$を近似的に求めてやろうという方法が<strong>変分近似</strong>です。</p><p>ここで$p(\mathbf{w}|\mathbf{X})$と近似関数$q(\mathbf{w})$の類似度合いの指標としてKLダイバージェンスを採用すると変分近似は、
$$ q_{opt.}(\mathbf{w})=\underset{q}{\operatorname{argmax}} \operatorname{KL}(q(\mathbf{w})||p(\mathbf{w}|\mathbf{X}))$$
の最適化問題として定式化できます。</p><p>しかしKLダイバージェンスに未知の関数である$p(\mathbf{w}|\mathbf{X})$が入っているため、このままでは$q_{opt.}$を求めることはできません。そこでこのKLダイバージェンスの最小化問題を、数学的なトリックを使って別の計算可能な量の最大化問題に書き換えることで間接的にKLダイバージェンスを最小化する関数を求めることを行います。ではどのようにするのでしょうか？</p><h2 id=elbo>ELBO
<a class=anchor href=#elbo>#</a></h2><p>ここで対数周辺尤度$\ln{p(\mathbf{X})}$を以下のように書き換えることができることに着目します。
$$
\begin{align}
\ln{p(\mathbf{X})} & = \ln{p(\mathbf{X})} \int_{\mathbf{w}} q(\mathbf{w}) d\mathbf{w}\newline
& = \int_{\mathbf{w}} q(\mathbf{w}) \ln\frac{p(\mathbf{X}, \mathbf{w})}{p(\mathbf{w} \vert \mathbf{X})} d\mathbf{w}\newline & = \int_{\mathbf{w}} q(\mathbf{w}) \ln \frac{p(\mathbf{X}, \mathbf{w})~q(\mathbf{w})}{p(\mathbf{w} \vert \mathbf{X}) ~q(\mathbf{w})} d\mathbf{w}\newline
& = \int_{\mathbf{w}} q(\mathbf{w}) \ln \frac{p(\mathbf{X}, \mathbf{w})}{q(\mathbf{w})} d\mathbf{w} + \int_{\mathbf{w}} q(\mathbf{w}) \ln \frac{q(\mathbf{w})}{p(\mathbf{w} \vert \mathbf{X})} d\mathbf{w}\newline
& = \mathcal{L}(\mathbf{X}) + \operatorname{KL}(q\vert \vert p)
\end{align}
$$
ここで１行目は
$$
\int_{\mathbf{w}} q(\mathbf{w}) d\mathbf{w} = 1
$$
を使っています。</p><p>ここで$\mathcal{L}(\mathbf{X})$は、我々が仮定する関数$q(\mathbf{w})$と確率モデル$p(\mathbf{X}, \mathbf{w})$から構成されているため計算が可能であることに注意してください。周辺尤度$p(\mathbf{X})$は確率モデルと観測データが与えられれば一意的に値が決まる量なので、上の式はこの$\mathcal{L}(\mathbf{X})$を最大化する$q(\mathbf{w})$を求めれば、それが自動的にKLダイバージェンスを最小化する$q(\mathbf{w})$を求めていることになることを示しています。つまり未知の関数を含むKLダイバージェンスを直接最小化できなくても、代わりに$\mathcal{L}(\mathbf{X})$を最大化すればその目的が達成できるのです！</p><p>この$\mathcal{L}(\mathbf{X})$をELBO(Evidence Lower BOund)と呼びます。</p><p>ここで$q(\mathbf{w})$をなんらかパラメータ$\boldsymbol{\alpha}$で特徴づけられる関数を仮定しているとすると、ELBO$\mathcal{L}(\mathbf{X})$の最大化、言い換えて$-\mathcal{L}(\mathbf{X})$の最小化はパラメータに対する偏微分$\frac{\partial \mathcal{L}}{\partial \boldsymbol{\alpha}}$を求めて勾配降下法で最適化計算を行っていくことができことになります。</p><p>以上、変分近似について解説してきました。要点は、</p><ol><li>事後確率$p(\mathbf{w}|\mathbf{X})$をパラメータ$\boldsymbol{\alpha}$で特徴づけられる関数（変分関数）$q(\mathbf{w})$で表現する。</li><li>求めたい事後確率分布に変分関数を似せる（つまりKLダイバージェンスを最小化する）ことで事後分布を近似的に求める。</li><li>ただしKLダイバージェンスを直接計算できないので、代わりにELBOを最大化することで最適な変分関数$q_{opt.}(\mathbf{w})$を求める。</li><li>ELBOの最大化は勾配降下法を用いて行う。
ということになります。</li></ol><p>次節以降でPyroで変分近似を行う例を具体的に示していきます。</p></article><footer class=book-footer><div class="flex flex-wrap justify-between"></div></footer><div class=book-comments></div><label for=menu-control class="hidden book-menu-overlay"></label></div><aside class=book-toc><div class=book-toc-content><nav id=TableOfContents><ul><li><a href=#変分推論の基礎>変分推論の基礎</a><ul><li><a href=#変分近似>変分近似</a></li><li><a href=#elbo>ELBO</a></li></ul></li></ul></nav></div></aside></main></body></html>
<!doctype html><html lang=en dir=ltr><head><meta name=generator content="Hugo 0.79.1"><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="MathJax.Hub.Config({ tex2jax: { inlineMath: [['$', '$'] ], displayMath: [ ['$$','$$'], [&#34;\\[&#34;,&#34;\\]&#34;] ] } });  モデル選択：周辺尤度最大化 #  本節では前多項式回帰を題材に、周辺尤度が最大となるモデルを選択するモデル選択手法をPyroで実際に行う方法を見ていきます。
■ 多項式回帰のモデリング #  多項式回帰としてモデリングする場合、説明変数$\mathbf{x}$と目的変数$y$の$N$個のセット、すなわち $$ \mathcal{D}={(\mathbf{x}_1, y_1),\cdots(\mathbf{x}_i, y_i)\cdots ,(\mathbf{x}_N, y_N)} $$ が与えられたとき、それぞれのサンプルに対して $$ y_i=\mathbf{w}^T\cdot \mathbf{x}_i + \epsilon_i \tag{1} $$ という関数型に従うことを仮定することに相当します。ここで$M$次元の多項式を仮定するとすると、$\mathbf{w} = (w_0, &mldr;, w_M)^T$, $\mathbf{x}_i = (x^0, x^1, x^2, &mldr;, x^M)^T = (1, x, x^2, &mldr;, x^M)^T$となります。
ここで$\epsilon_i$を平均0、標準偏差$\sigma$のガウス分布に従う、つまり $$ \epsilon \sim \mathcal{N}(\epsilon|0, \sigma) \tag{2} $$ と仮定すると、(1)、(2)式をまとめることで目的変数の確率分布$p(y_i|\mathbf{x}_i,\mathbf{w})$は $$ p(y_i|\mathbf{x}_i,\mathbf{w})=\mathcal{N}(y_i|\mathbf{w}\cdot\mathbf{x}_i, \sigma)\tag{3} $$ のように書けることが分かります。 簡単のため、今回は観測誤差の広がり度合いを示す$\sigma$は既知の定数とすると1、同時確率分布は $$ p(\mathbf{X},\mathbf{Y},\mathbf{w})=p(\mathbf{w})\prod_i\mathcal{N}(y_i|\mathbf{w}\cdot\mathbf{x}_i)p(\mathbf{x}_i)\tag{4} $$ という具体的な形に書くことができます。"><meta name=theme-color content="#FFFFFF"><meta property="og:title" content="モデル選択：周辺尤度最大化"><meta property="og:description" content><meta property="og:type" content="website"><meta property="og:url" content="https://pyro-book.data-hacker.net/docs/model_selection_01/"><title>モデル選択：周辺尤度最大化 | Pyroで実践するベイズ機械学習</title><link rel=manifest href=https://pyro-book.data-hacker.net/manifest.json><link rel=icon href=https://pyro-book.data-hacker.net/favicon.png type=image/x-icon><link rel=stylesheet href=https://pyro-book.data-hacker.net/book.min.6c7c6446dfdee7c8c933e9bbc6e80ee3ed6c913b2a59519f2092c3c6a9d63e55.css integrity="sha256-bHxkRt/e58jJM+m7xugO4+1skTsqWVGfIJLDxqnWPlU="><script defer src=https://pyro-book.data-hacker.net/en.search.min.95d344ac1e77adc21bb402b1529a7be64075da5ed47936b3e33f06ed7135f2be.js integrity="sha256-ldNErB53rcIbtAKxUpp75kB12l7UeTaz4z8G7XE18r4="></script><script defer src=https://pyro-book.data-hacker.net/sw.min.74a8bb07f0bee86d6bb9a2750f073f14d93c7e4512f28860370cfd879e9719b4.js integrity="sha256-dKi7B/C+6G1ruaJ1Dwc/FNk8fkUS8ohgNwz9h56XGbQ="></script><link rel=alternate type=application/rss+xml href=https://pyro-book.data-hacker.net/docs/model_selection_01/index.xml title=Pyroで実践するベイズ機械学習></head><body dir=ltr><input type=checkbox class="hidden toggle" id=menu-control>
<input type=checkbox class="hidden toggle" id=toc-control><main class="container flex"><aside class=book-menu><div class=book-menu-content><nav><h2 class=book-brand><a href=https://pyro-book.data-hacker.net/><span>Pyroで実践するベイズ機械学習</span></a></h2><div class=book-search><input type=text id=book-search-input placeholder=Search aria-label=Search maxlength=64 data-hotkeys=s/><div class="book-search-spinner hidden"></div><ul id=book-search-results></ul></div><ul><li><a href=https://pyro-book.data-hacker.net/docs/what_is_pyro/>Pyroとは</a></li><li><a href=https://pyro-book.data-hacker.net/docs/pyro_modeling/>Pyroによる確率モデリング</a></li><li><a href=https://pyro-book.data-hacker.net/docs/dist_basic/>確率分布の取り扱い</a></li><li><a href=https://pyro-book.data-hacker.net/docs/bayes_learning_basic/>ベイズ学習の枠組み</a></li><li><a href=https://pyro-book.data-hacker.net/docs/vi_basic/>変分推論の基礎</a></li><li><a href=https://pyro-book.data-hacker.net/docs/pyro_vi/>Pyroでの変分推論</a></li><li><a href=https://pyro-book.data-hacker.net/docs/mle_map/>MAP推定と最尤推定</a></li><li><a href=https://pyro-book.data-hacker.net/docs/linear_regression/>ベイズ線形回帰</a></li><li><a href=https://pyro-book.data-hacker.net/docs/model_selection_01/ class=active>モデル選択:周辺尤度最大化</a><br><br></li></ul><ul><li><a href=https://github.com/a-mitani/bayes-pyro target=_blank rel=noopener>Github</a></li></ul></nav><script>(function(){var menu=document.querySelector("aside.book-menu nav");addEventListener("beforeunload",function(event){localStorage.setItem("menu.scrollTop",menu.scrollTop);});menu.scrollTop=localStorage.getItem("menu.scrollTop");})();</script></div></aside><div class=book-page><header class=book-header><div class="flex align-center justify-between"><label for=menu-control><img src=https://pyro-book.data-hacker.net/svg/menu.svg class=book-icon alt=Menu></label>
<strong>モデル選択：周辺尤度最大化</strong>
<label for=toc-control><img src=https://pyro-book.data-hacker.net/svg/toc.svg class=book-icon alt="Table of Contents"></label></div><aside class="hidden clearfix"><nav id=TableOfContents><ul><li><a href=#モデル選択周辺尤度最大化>モデル選択：周辺尤度最大化</a><ul><li><a href=#-多項式回帰のモデリング>■ 多項式回帰のモデリング</a></li><li><a href=#-多項式回帰の実装>■ 多項式回帰の実装</a><ul><li><a href=#確率モデル>確率モデル</a></li><li><a href=#変分関数>変分関数</a></li><li><a href=#推論>推論</a></li><li><a href=#学習結果>学習結果</a></li></ul></li><li><a href=#-モデル選択周辺尤度最大化>■ モデル選択：周辺尤度最大化</a><ul><li><a href=#周辺尤度>周辺尤度</a></li><li><a href=#対数周辺尤度の計算>対数周辺尤度の計算</a></li></ul></li></ul></li></ul></nav></aside></header><article class=markdown><script type=text/javascript async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML"></script><script type=text/x-mathjax-config>
 MathJax.Hub.Config({
 tex2jax: {
 inlineMath: [['$', '$'] ],
 displayMath: [ ['$$','$$'], ["\\[","\\]"] ]
 }
 });
</script><h1 id=モデル選択周辺尤度最大化>モデル選択：周辺尤度最大化
<a class=anchor href=#%e3%83%a2%e3%83%87%e3%83%ab%e9%81%b8%e6%8a%9e%e5%91%a8%e8%be%ba%e5%b0%a4%e5%ba%a6%e6%9c%80%e5%a4%a7%e5%8c%96>#</a></h1><p>本節では前多項式回帰を題材に、周辺尤度が最大となるモデルを選択するモデル選択手法をPyroで実際に行う方法を見ていきます。</p><h2 id=-多項式回帰のモデリング>■ 多項式回帰のモデリング
<a class=anchor href=#-%e5%a4%9a%e9%a0%85%e5%bc%8f%e5%9b%9e%e5%b8%b0%e3%81%ae%e3%83%a2%e3%83%87%e3%83%aa%e3%83%b3%e3%82%b0>#</a></h2><p>多項式回帰としてモデリングする場合、説明変数$\mathbf{x}$と目的変数$y$の$N$個のセット、すなわち
$$
\mathcal{D}={(\mathbf{x}_1, y_1),\cdots(\mathbf{x}_i, y_i)\cdots ,(\mathbf{x}_N, y_N)}
$$
が与えられたとき、それぞれのサンプルに対して
$$
y_i=\mathbf{w}^T\cdot \mathbf{x}_i + \epsilon_i \tag{1}
$$
という関数型に従うことを仮定することに相当します。ここで$M$次元の多項式を仮定するとすると、$\mathbf{w} = (w_0, &mldr;, w_M)^T$, $\mathbf{x}_i = (x^0, x^1, x^2, &mldr;, x^M)^T = (1, x, x^2, &mldr;, x^M)^T$となります。</p><p>ここで$\epsilon_i$を平均0、標準偏差$\sigma$のガウス分布に従う、つまり
$$
\epsilon \sim \mathcal{N}(\epsilon|0, \sigma) \tag{2}
$$
と仮定すると、(1)、(2)式をまとめることで目的変数の確率分布$p(y_i|\mathbf{x}_i,\mathbf{w})$は
$$
p(y_i|\mathbf{x}_i,\mathbf{w})=\mathcal{N}(y_i|\mathbf{w}\cdot\mathbf{x}_i, \sigma)\tag{3}
$$
のように書けることが分かります。
簡単のため、今回は観測誤差の広がり度合いを示す$\sigma$は既知の定数とすると<sup id=fnref:1><a href=#fn:1 class=footnote-ref role=doc-noteref>1</a></sup>、同時確率分布は
$$
p(\mathbf{X},\mathbf{Y},\mathbf{w})=p(\mathbf{w})\prod_i\mathcal{N}(y_i|\mathbf{w}\cdot\mathbf{x}_i)p(\mathbf{x}_i)\tag{4}
$$
という具体的な形に書くことができます。</p><p>またこの同時確率をグラフィカルモデルで記述すれば以下の図のようになります。</p><center><img src=general_regression.png width=400></center><p>このモデルに従い、前節の線形回帰モデルを拡張することで多項式回帰をPyroで実装していくことにします。なお以降では下記のコードを実行されている前提で話を進めていきます。</p><p><strong>※プログラムコードの全体は
<a href=https://github.com/a-mitani/pyro_code_examples/blob/main/model_selection.ipynb>Github</a>上に公開しています。</strong></p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=color:#f92672>import</span> matplotlib.pyplot <span style=color:#f92672>as</span> plt
<span style=color:#f92672>import</span> numpy <span style=color:#f92672>as</span> np

<span style=color:#f92672>import</span> torch
<span style=color:#f92672>from</span> torch.distributions <span style=color:#f92672>import</span> constraints

<span style=color:#f92672>import</span> pyro
<span style=color:#f92672>import</span> pyro.distributions <span style=color:#f92672>as</span> dist
<span style=color:#f92672>from</span> pyro.infer <span style=color:#f92672>import</span> SVI, Trace_ELBO
<span style=color:#f92672>from</span> pyro.infer <span style=color:#f92672>import</span> Predictive
<span style=color:#f92672>from</span> pyro.infer.autoguide <span style=color:#f92672>import</span> AutoDiagonalNormal

np<span style=color:#f92672>.</span>random<span style=color:#f92672>.</span>seed(<span style=color:#ae81ff>1</span>)
pyro<span style=color:#f92672>.</span>set_rng_seed(<span style=color:#ae81ff>1</span>)
</code></pre></div><p>また、下記のコードで生成されるToyデータセットを例に進めていきます。</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>create_poly_data</span>(coef, size, x_max, sigma):
    x <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>random<span style=color:#f92672>.</span>rand(size) <span style=color:#f92672>*</span> x_max
    y <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>zeros(size)
    <span style=color:#66d9ef>for</span> i <span style=color:#f92672>in</span> range(len(coef)):
        y <span style=color:#f92672>+=</span> coef[i] <span style=color:#f92672>*</span> x <span style=color:#f92672>**</span> i
    y <span style=color:#f92672>+=</span> np<span style=color:#f92672>.</span>random<span style=color:#f92672>.</span>normal(scale<span style=color:#f92672>=</span>sigma, size<span style=color:#f92672>=</span>size)
    <span style=color:#66d9ef>return</span> x, y

COEF <span style=color:#f92672>=</span> [<span style=color:#ae81ff>2</span>, <span style=color:#f92672>-</span><span style=color:#ae81ff>1.5</span>, <span style=color:#ae81ff>0.6</span>, <span style=color:#f92672>-</span><span style=color:#ae81ff>0.05</span>]
X_MAX <span style=color:#f92672>=</span> <span style=color:#ae81ff>10</span>
SIGMA <span style=color:#f92672>=</span> <span style=color:#ae81ff>1.0</span>

x, y <span style=color:#f92672>=</span> create_poly_data(COEF, <span style=color:#ae81ff>100</span>, X_MAX, SIGMA)
train <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>tensor(np<span style=color:#f92672>.</span>array([x, y])<span style=color:#f92672>.</span>T, dtype<span style=color:#f92672>=</span>torch<span style=color:#f92672>.</span>float)

plt<span style=color:#f92672>.</span>scatter(x, y)
plt<span style=color:#f92672>.</span>xlabel(<span style=color:#e6db74>&#39;x&#39;</span>)
plt<span style=color:#f92672>.</span>ylabel(<span style=color:#e6db74>&#39;y&#39;</span>)
plt<span style=color:#f92672>.</span>show()
</code></pre></div><p>生成されたデータは以下のグラフのようになります。このToyデータセットをもとにベイズ線形回帰を行っていきましょう。</p><center><img src=poly_toy_data.png width=400></center><h2 id=-多項式回帰の実装>■ 多項式回帰の実装
<a class=anchor href=#-%e5%a4%9a%e9%a0%85%e5%bc%8f%e5%9b%9e%e5%b8%b0%e3%81%ae%e5%ae%9f%e8%a3%85>#</a></h2><h3 id=確率モデル>確率モデル
<a class=anchor href=#%e7%a2%ba%e7%8e%87%e3%83%a2%e3%83%87%e3%83%ab>#</a></h3><p>(3)式の確率モデルをPyroを用いた関数として定義します。</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>model</span>(x, y, model_dim, sigma):
    param_dim <span style=color:#f92672>=</span> model_dim <span style=color:#f92672>+</span> <span style=color:#ae81ff>1</span>
    w <span style=color:#f92672>=</span> pyro<span style=color:#f92672>.</span>sample(<span style=color:#e6db74>&#34;w&#34;</span>, dist<span style=color:#f92672>.</span>Normal(<span style=color:#ae81ff>0.0</span>, <span style=color:#ae81ff>10.0</span>)<span style=color:#f92672>.</span>expand([param_dim])<span style=color:#f92672>.</span>to_event(<span style=color:#ae81ff>1</span>))
    mean <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>zeros_like(x)
    <span style=color:#66d9ef>for</span> i <span style=color:#f92672>in</span> range(param_dim):
        mean <span style=color:#f92672>+=</span> w[i] <span style=color:#f92672>*</span> (x <span style=color:#f92672>**</span> i)
    <span style=color:#66d9ef>with</span> pyro<span style=color:#f92672>.</span>plate(<span style=color:#e6db74>&#34;data&#34;</span>, len(x)):
        pyro<span style=color:#f92672>.</span>sample(<span style=color:#e6db74>&#34;obs&#34;</span>, dist<span style=color:#f92672>.</span>Normal(mean, sigma), obs<span style=color:#f92672>=</span>y)
</code></pre></div><p>実装コードを詳しくみていきます。</p><p>2行目で今回学習したいパラメータ $\mathbf{w}$の次元数を定義しています。<code>model_dim</code>はモデルの次元、つまり何次多項式としてモデリングするかの変数であり、<code>param_dim</code>は定数項の分を加えた<code>model_dim+1</code>となります。（なお、後々のモデル選択の話の際に便利なようにここで<code>model_dim</code>はモデルの引数として外から指定できるように実装しています。）</p><p>3行目はパラメータ$\mathbf{w}$の事前分布を多次元正規分布として定義しています。そして全ての成分の平均値は<code>0.0</code>、標準偏差が<code>10.0</code>とし、かつ各成分は独立（つまり分散共分散行列の非対角成分はゼロ）と仮定していることになります。また<code>expand</code>や<code>to_event</code>関数については
<a href=https://pyro-book.data-hacker.net/docs/dist_basic/>確率分布の取り扱い</a>の節を参照してください。</p><p>4~8行目は目的変数$y_i$を定義しており、(1)式より観測平均$\mathbf{w}^T\cdot \mathbf{x}_i$に対して、各観測個別に平均ゼロ、標準偏差$\sigma$の観測ノイズが乗った値と仮定おり、その仮定をそのまま実装しているのがわかるでしょう。</p><h3 id=変分関数>変分関数
<a class=anchor href=#%e5%a4%89%e5%88%86%e9%96%a2%e6%95%b0>#</a></h3><p>次に変分関数を定義しますが、ここではPyroの<code>AutoGuide</code>クラスを利用して実装すると以下のような1行で済みます。</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python>guide <span style=color:#f92672>=</span> AutoDiagonalNormal(model)
</code></pre></div><p><code>AutoGude</code>クラスは引数の<code>model</code>内で指定された名前付きの確率変数（ただし<code>obs</code>指定されたものを除く）がベイズ学習対象と自動認識し、各変数に対して規定された変分関数を自動生成するものになります。ここでは<code>AutoGuide</code>のサブクラスである<code>AutoDiagonalNormal</code>を用いており、これはすなわち<code>model</code>内の<code>w</code>の変分関数を分散共分散行列の対角成分がゼロの多次元正規関数の形をとる変分関数を規定した事に相当します。</p><h3 id=推論>推論
<a class=anchor href=#%e6%8e%a8%e8%ab%96>#</a></h3><p>確率モデルと変分関数を規定したら、あとはELBOを最大化するようなパラメータを求める学習を行うだけです。下記のコードのように実装が出来ます。ここでは今回の学習対象のtoyデータを見ると微分値がゼロとなるようなのが２点程度ありそうなので、３次多項式で表現出来そうかな、とあたりを付けて<code>model_dim=3</code>として学習をしています。</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python>model_dim <span style=color:#f92672>=</span> <span style=color:#ae81ff>3</span>
optimizer <span style=color:#f92672>=</span> pyro<span style=color:#f92672>.</span>optim<span style=color:#f92672>.</span>Adam({<span style=color:#e6db74>&#34;lr&#34;</span>: <span style=color:#f92672>.</span><span style=color:#ae81ff>01</span>})
svi <span style=color:#f92672>=</span> SVI(model, guide, optimizer, loss<span style=color:#f92672>=</span>Trace_ELBO())

x, y <span style=color:#f92672>=</span> train[:, <span style=color:#ae81ff>0</span>], train[:, <span style=color:#ae81ff>1</span>]
pyro<span style=color:#f92672>.</span>clear_param_store()
num_iters <span style=color:#f92672>=</span> <span style=color:#ae81ff>50000</span>
iter_nums <span style=color:#f92672>=</span> []
losses <span style=color:#f92672>=</span> []
<span style=color:#66d9ef>for</span> i <span style=color:#f92672>in</span> range(num_iters):
    loss <span style=color:#f92672>=</span> svi<span style=color:#f92672>.</span>step(x, y, model_dim, SIGMA)
    iter_nums<span style=color:#f92672>.</span>append(i <span style=color:#f92672>+</span> <span style=color:#ae81ff>1</span>)
    losses<span style=color:#f92672>.</span>append(loss)
    <span style=color:#66d9ef>if</span> i <span style=color:#f92672>%</span> (num_iters <span style=color:#f92672>/</span> <span style=color:#ae81ff>10</span>) <span style=color:#f92672>==</span> <span style=color:#ae81ff>0</span>:
        <span style=color:#66d9ef>print</span>(<span style=color:#e6db74>&#34;Elbo loss: {}&#34;</span><span style=color:#f92672>.</span>format(loss))

plt<span style=color:#f92672>.</span>plot(iter_nums, losses)
plt<span style=color:#f92672>.</span>xscale(<span style=color:#e6db74>&#34;log&#34;</span>)
plt<span style=color:#f92672>.</span>yscale(<span style=color:#e6db74>&#34;log&#34;</span>)
plt<span style=color:#f92672>.</span>xlabel(<span style=color:#e6db74>&#39;iteration num&#39;</span>)
plt<span style=color:#f92672>.</span>ylabel(<span style=color:#e6db74>&#39;Loss (= -ELBO)&#39;</span>)
plt<span style=color:#f92672>.</span>show()
</code></pre></div><p>実行結果のLossの変化は下図のようになり、十分収束してそうなのがみて取れるかと思います。</p><center><img src=elbo.png width=400></center><h3 id=学習結果>学習結果
<a class=anchor href=#%e5%ad%a6%e7%bf%92%e7%b5%90%e6%9e%9c>#</a></h3><p>今回学習して得られた学習パラメータの結果を表示してみます。</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=color:#66d9ef>for</span> name, value <span style=color:#f92672>in</span> pyro<span style=color:#f92672>.</span>get_param_store()<span style=color:#f92672>.</span>items():
    <span style=color:#66d9ef>print</span>(name, value)

<span style=color:#75715e>## Output</span>
<span style=color:#75715e># AutoDiagonalNormal.loc Parameter containing:</span>
<span style=color:#75715e># tensor([ 2.5502, -1.8158,  0.6533, -0.0535], requires_grad=True)</span>
<span style=color:#75715e># AutoDiagonalNormal.scale tensor([0.0949, 0.0155, 0.0024, 0.0003], grad_fn=&lt;AddBackward0&gt;)</span>
</code></pre></div><p><code>AutoDiagonalNormal</code>を利用したため、今回の推定パラメータである平均と標準偏差はそれぞれ<code>AutoDiagonalNormal.loc</code>、<code>AutoDiagonalNormal.scale</code>というKeyで格納されているのがみて取れます。</p><p>数値だけでは学習結果の妥当性が見えないので実際に今回推定されたパラメータで、前節と同様に回帰曲線と観測値の90%信頼区間をプロットしてみると、それぞれ以下の左と右の図のようになります<sup id=fnref:2><a href=#fn:2 class=footnote-ref role=doc-noteref>2</a></sup>。今回の結果が観測データを上手く表現出来ていそうだというのがみて取れるかと思います。</p><center><img src=ci_90_poly.png width=800></center><h2 id=-モデル選択周辺尤度最大化>■ モデル選択：周辺尤度最大化
<a class=anchor href=#-%e3%83%a2%e3%83%87%e3%83%ab%e9%81%b8%e6%8a%9e%e5%91%a8%e8%be%ba%e5%b0%a4%e5%ba%a6%e6%9c%80%e5%a4%a7%e5%8c%96>#</a></h2><p>さて、今回のトイモデルに対しては３次の多項式で回帰することでデータを上手く表現出来ることが定性的に見て取れましたが、他のモデルの方が観測データをより上手く表現できる可能性も捨てきれません。例えば２次多項式や４次多項式での回帰モデルの方が今回のデータをより上手く表現できる可能性があります。そのことを確認するためにそれぞれのモデルで回帰したうえで結果を比較し、最も上手く観測データを表現できるモデルを探すのが<strong>モデル選択</strong>です。</p><p>しかしモデル選択のためにはモデル同士でどちらがより上手く観測データを表現出来ているかを比較するための定量的な指標が必要になります。ベイズ学習ではしばしば<strong>周辺尤度(Marginal Likelihood)</strong> <sup id=fnref:3><a href=#fn:3 class=footnote-ref role=doc-noteref>3</a></sup>と呼ばれる量が最も大きくなるモデルを選択することを行います（周辺尤度最大化）。</p><h3 id=周辺尤度>周辺尤度
<a class=anchor href=#%e5%91%a8%e8%be%ba%e5%b0%a4%e5%ba%a6>#</a></h3><p>改めて観測データ$\mathbf{X}$, $\mathbf{Y}$とパラメータ$\mathbf{w}$の同時確率を考えると、事後確率との関係から
$$
p(\mathbf{X}, \mathbf{Y}, \mathbf{w})=p(\mathbf{X}, \mathbf{Y}| \mathbf{w})p(\mathbf{w})
$$
と書けます。この式を両辺$\mathbf{w}$について積分すると、
$$
p(\mathbf{X}, \mathbf{Y})=\int p(\mathbf{X}, \mathbf{Y}| \mathbf{w})p(\mathbf{w})d\mathbf{w}
$$
この$p(\mathbf{X}, \mathbf{Y})$を<strong>周辺尤度</strong>と呼びます。右辺を見ると、周辺尤度はベイズ学習により得られた確率分布$p(\mathbf{w})$のもとでの、観測データの生起確率$p(\mathbf{X}, \mathbf{Y})$の期待値を表していることがわかります。そのためこの周辺尤度が最も大きいモデルを選択することはそのモデルから観測データが最も生起しやすいことを示している事になります。
また周辺尤度の対数（対数周辺尤度）をとっても、モデル間の周辺尤度の大小関係は変わらないため、しばしば以下の対数周辺尤度を求めてその大小関係からモデル選択が行われます。
$$
\ln p(\mathbf{X}, \mathbf{Y})=\int \ln p(\mathbf{X}, \mathbf{Y}| \mathbf{w})p(\mathbf{w})d\mathbf{w}
$$</p><h3 id=対数周辺尤度の計算>対数周辺尤度の計算
<a class=anchor href=#%e5%af%be%e6%95%b0%e5%91%a8%e8%be%ba%e5%b0%a4%e5%ba%a6%e3%81%ae%e8%a8%88%e7%ae%97>#</a></h3><p>具体的に今回の多項式回帰モデルでの対数周辺尤度を求めていきましょう。ここで上式右辺の積分は一般的に解析的に求めるのは難しいため、今回はモンテカルロ積分で近似計算する方法をとることにします。すると対数周辺尤度は以下のように書けます。
$$
\begin{align}
\ln p(\mathbf{X}, \mathbf{Y})
&= \int \ln p(\mathbf{X}, \mathbf{Y}| \mathbf{w})p(\mathbf{w})d\mathbf{w}\newline
&\approx \frac{1}{T} \sum_{t=1}^{T} \ln p(\mathbf{X}, \mathbf{Y}| \mathbf{w}^{(t)})\newline
&= \frac{1}{T} \sum_{t=1}^{T} \sum_{i=1}^{N}\ln \mathcal{N}(y_i|\mathbf{w}^{(t)}\cdot \mathbf{x}_i, \sigma)
\end{align}
$$
ここで2行目は積分をモンテカルロ積分で近似しており、$\mathbf{w}^{(t)}$は回帰パラメータの確率分布$p(\mathbf(w))$からのサンプリング値を示しています。また3行目は(3)式を用いています。</p><p>この式に従って対数周辺尤度を求めるコードをPyroを用いて実装すると以下のようになります。</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python>SAMPLE_NUM <span style=color:#f92672>=</span> <span style=color:#ae81ff>1000</span>
predictive <span style=color:#f92672>=</span> Predictive(model<span style=color:#f92672>=</span>model, guide<span style=color:#f92672>=</span>guide, num_samples<span style=color:#f92672>=</span>SAMPLE_NUM, return_sites<span style=color:#f92672>=</span>[<span style=color:#e6db74>&#34;w&#34;</span>])
predict_samples <span style=color:#f92672>=</span> predictive<span style=color:#f92672>.</span>get_samples(x, None, model_dim, SIGMA)

predicted_mean <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>zeros((SAMPLE_NUM, len(x)))
<span style=color:#66d9ef>for</span> i <span style=color:#f92672>in</span> range(model_dim <span style=color:#f92672>+</span> <span style=color:#ae81ff>1</span>):
    predicted_mean <span style=color:#f92672>+=</span> predict_samples[<span style=color:#e6db74>&#34;w&#34;</span>][:, :, i] <span style=color:#f92672>*</span> (x <span style=color:#f92672>**</span> i)

sum_log_prob <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>zeros(SAMPLE_NUM)
<span style=color:#66d9ef>for</span> i <span style=color:#f92672>in</span> range(len(x)):
    normal_1d <span style=color:#f92672>=</span> dist<span style=color:#f92672>.</span>Normal(predicted_mean[:, i], SIGMA)
    sum_log_prob <span style=color:#f92672>=</span> normal_1d<span style=color:#f92672>.</span>log_prob(y[i])
log_marginal_likelihood <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>mean(sum_log_prob)
<span style=color:#66d9ef>print</span>(<span style=color:#e6db74>&#34;log_marginal_likelihood = &#34;</span>, log_marginal_likelihood<span style=color:#f92672>.</span>item())

<span style=color:#75715e>## Output</span>
<span style=color:#75715e># log_marginal_likelihood =  -0.9372764825820923</span>
</code></pre></div><p>コードを詳しく見ていきます。</p><p>まず1~3行目では今回学習したパラメータの確率分布$p(\mathbf{w})$から$\mathbf{w}$を1,000回サンプリングし、5~7行目ではそのサンプル値と観測目的変数$x_i$からy_iの平均を求めています。9~13行目は$y_i$の平均値と観測ノイズの標準偏差$\sigma$で特徴づけられる正規分布から実際の観測値S$y_i$が得られる確率密度を求めて各サンプルについての和をとることで対数尤度分布を計算しています。</p><p>下の図は上記の一連のコードで、多項式回帰の次元数を定義する<code>model_dim</code>の値を変更することで、1次元〜5次元の多項式回帰モデルでの回帰曲線の90%信頼区間をプロットしたものと、回帰モデルの次元数と対数周辺尤度の大きさの関係をプロットしたものになります<sup id=fnref:4><a href=#fn:4 class=footnote-ref role=doc-noteref>4</a></sup>。3次または4次多項式のモデルが最も対数周辺尤度が大きく今回の観測データを表現するのに適しているのが見て取れます。モデルとして1次元や2次元での回帰では表現力が低く、また５次では複雑なモデルが故に観測誤差にも合せ込もうとしてしまっているのが見て取れるでしょう。</p><center><img src=each_dim.png width=800></center><section class=footnotes role=doc-endnotes><hr><ol><li id=fn:1 role=doc-endnote><p>前節と同様にすることで、$\sigma$を推定対象にすることも可能です。 <a href=#fnref:1 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:2 role=doc-endnote><p>ここではサンプルコードは省略します。Github上のコード例を参照してください。 <a href=#fnref:2 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:3 role=doc-endnote><p>モデルエビデンス(Model Evidence)とも呼ばれることもあります。またこの量の対数をとり符号を逆転した量は、自由エネルギー(Free Energy）とも呼ばれます。 <a href=#fnref:3 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:4 role=doc-endnote><p>5次多項式は収束が遅いため、サンプルコードに最適化時に学習率の減衰の機構を入れて求めています。 <a href=#fnref:4 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li></ol></section></article><footer class=book-footer><div class="flex flex-wrap justify-between"></div></footer><div class=book-comments></div><label for=menu-control class="hidden book-menu-overlay"></label></div><aside class=book-toc><div class=book-toc-content><nav id=TableOfContents><ul><li><a href=#モデル選択周辺尤度最大化>モデル選択：周辺尤度最大化</a><ul><li><a href=#-多項式回帰のモデリング>■ 多項式回帰のモデリング</a></li><li><a href=#-多項式回帰の実装>■ 多項式回帰の実装</a><ul><li><a href=#確率モデル>確率モデル</a></li><li><a href=#変分関数>変分関数</a></li><li><a href=#推論>推論</a></li><li><a href=#学習結果>学習結果</a></li></ul></li><li><a href=#-モデル選択周辺尤度最大化>■ モデル選択：周辺尤度最大化</a><ul><li><a href=#周辺尤度>周辺尤度</a></li><li><a href=#対数周辺尤度の計算>対数周辺尤度の計算</a></li></ul></li></ul></li></ul></nav></div></aside></main></body></html>
---
weight: 1
---
# ベイズ学習の枠組み

## ベイズ学習は確率分布を学習する
　ベイズ学習は、観測された事象（データ）{{<katex>}}D{{</katex>}}が得られたという条件のもとで未知の変数{{<katex>}}W{{</katex>}}の確率分布{{<katex>}}P(W){{</katex>}}を推論する、すなわち事後分布{{<katex>}}P(W|D){{</katex>}}を求める作業になります。

　例えば、赤玉と白玉が入っている箱がありそれぞれの色の個数の割合{{<katex>}}\theta{{</katex>}}が未知である場合、その{{<katex>}}\theta{{</katex>}}の確率分布{{<katex>}}P(\theta){{</katex>}}を箱から無作為に取り出した玉の色のデータ{{<katex>}}D{{</katex>}}を得られた事実をもとに推論する、すなわち{{<katex>}}P(\Theta|D){{</katex>}}を計算するというようなものになります。

もう１つの例としては線形回帰{{<katex>}} y=\boldsymbol{w} \cdot \boldsymbol{x} +b{{</katex>}}の学習パラメータ{{<katex>}}\boldsymbol{w}{{</katex>}}、{{<katex>}}b{{</katex>}}を未知の変数としその確率分布{{<katex>}}P(\boldsymbol{w}){{</katex>}}、{{<katex>}}P(b){{</katex>}}を観測データ{{<katex>}}D{{</katex>}}から求める、すなわち{{<katex>}}P(\boldsymbol{w}|D){{</katex>}}、{{<katex>}}P(b|D){{</katex>}}を計算するというものが挙げられます。

## 学習への２つのステップ
一般的にベイズ学習は以下の２つのStepで行っていくといえます。
1. 確率モデルの構築：グラフィカルモデルなどを利用しながら、事象の同時確率を定式化する。
2. 推論：上で定式化した同時確率分布と、その未知のパラメータに対する周辺確率から事後確率を求める

それぞれのステップについて少し詳しく見ていきましょう。
### ▼ Step1：確率モデルの構築
まず、着目する事象が確率的なプロセスから発生するものだという仮定を置き、そのプロセスをモデル化することから始めます。モデル化には事象間もしくは変数間の関係性をグラフ表現するグラフィカルモデルが有用です。
また事象の確率プロセスがモデル化できるとそれはすなわち変数間の同時確率分布を定式化することになります。
例えば上の赤玉と白玉の割合の例の場合、下左図のようにモデル化することができます。
下図はすなわち、赤玉と白玉の割合自体も確率的に決まっており（{{<katex>}}P(\Theta){{</katex>}}）、そこから取り出される玉の色は玉の割合が{{<katex>}}\theta{{</katex>}}と決まった上での条件付き確率で表されるとモデル化していることになります。
また、線形回帰の例の場合も下右図のようにモデル化することができます(*)。未知のパラメータが確率的に決まっているものとし（{{<katex>}}P(\boldsymbol{w}){{</katex>}}、{{<katex>}}P(b){{</katex>}}）、かつ、データ自体も確率的に発生し、目的変数はそれら確率変数が決定された上での条件付き確率として表現できるというモデリングを行っています。

{{< figure src="20210201.001.png" alt="画像1" >}}

{{< hint info >}}
**Hint**  
図内の四角形で囲った部分は、グラフィカルモデル のプレート図の表現で、独立のN個あるデータを１つにまとめたことを意味しています。
{{< /hint >}}

### ▼ Step2：推論
Step1で確率モデルが構築できたら、そのモデルと観測データから未知のパラメータの確率分布を推論します。これはすなわち冒頭での話のとおり、観測データ{{<katex>}}D{{</katex>}}を得た条件下で未知のパラメータがとる条件付き確率{{<katex>}}P(W|D){{</katex>}}を推論することに相当します。
ではこの{{<katex>}}P(W|D){{</katex>}}はどう計算すれば良いのかを考えていきましょう。条件付き確率の定義から
{{< katex display >}}P(W|D)=\frac{P(W,D)}{P(D)}=\frac{P(W,X)}{\sum_W P(W,X)}{{< /katex>}}
と書き換えられます。
分子の同時確率はStep1の確率モデルの構築ができた時点で定式化されており求めることができるし、分母は未知のパラメータの取りえる値全てに関して同時確率を足し合わす（周辺化する）ことで求められます。

つまりこの式は、どのような確率モデルの例であっても、同時確率とその未知のパラメータに対する周辺分布を計算することで事後分布{{<katex>}}P(W|D){{</katex>}}を推論可能であるということを示していることになります。

実際の複雑な確率モデルを扱う場合、周辺確率を求めるのに非常にコストがかかるためサンプリングや変分法と呼ばれる近似手法によって事後分布を計算するケースが多いですが、**おおもとのベイズ学習の発想は「同時確率とその未知のパラメータに対する周辺分布から事後分布を計算する」ということにある**ことは覚えておいた方が良いでしょう。

## 学習の具体例
では、手で計算できるレベルの非常に単純なモデルを例に、同時確率とその周辺確率から実際にベイズ推論を行ってみたいと思います。

ここでは次のような「箱の中のボールの数を推論する」例を考えていきます。

ある箱の中にボールが３つ入っている。ボールの色は赤か白のどちらかだが、どの色が何個入っているかはわからない。ここで箱の中からランダムに１つボールを取り出しそのボールの色を確認後箱の中に戻すという操作を行う。
【ケースA】１回の試行で「白」が出た場合
【ケースB】３回の試行で「白→赤→白」が出た場合
の２つの場合で箱の中の白ボールの数がどのように推論できるかを見ていこう。

### 【ケースA】１回の試行で「白」が出た場合

上のように、確率モデルの構築→推論とステップを踏んで進めていこう。
#### 確率モデルの構築
この場合の事象は下図のようなグラフィカルモデルで表せる。ここで{{<katex>}}W=\{0, 1, 2, 3\}{{</katex>}}は白玉の数を示す確率変数で、{{<katex>}}W{{</katex>}}の値により試行時に取り出される玉の色{{<katex>}}X=\{r, w\}{{</katex>}}の確率が決まるというモデルになっている。

{{< figure src="20210202.001.png" alt="画像2" >}}


また、簡単な確率の考察からそれぞれの確率は下表のようになる。ここで箱の中の玉の数は何の情報もないため等確率で発生するものとして{{<katex>}}P_0(W)=1/4{{</katex>}}、ここで{{<katex>}}W=\{0,1, 2, 3\}{{</katex>}}としている。この{{<katex>}}P_0(W){{</katex>}}を事前確率という。

{{< figure src="20210202.002.png" alt="画像3" >}}

#### 推論

「１回の試行で白ボールを取り出した」というデータが確定したもとでの白ボールの数を推論したいので、求めたいのは事後確率{{<katex>}}P(W|X_1=w){{</katex>}}であり、条件付き確率の定義から
{{<katex display>}}P(W|X_1=w)=\frac{P(W,X_1=w)}{P(X_1=w)}=\frac{P(W,X_1=w)}{\sum_W{P(W, X_1=w)}}\tag{1}{{</katex>}}
と書ける。上記の最左辺の分子と分母はこれまでの情報で計算できることがわかると思う。それぞれ求めていってみよう。
分子の同時確率は{{<katex>}}P(W, X_1)=P(X_1|W)P(W){{</katex>}}であり、{{<katex>}}P(W)_0=P(W){{</katex>}}とすると、
上の表から以下のように計算できる。
{{<katex display>}}\begin{cases}P(W=0, X_1=w) & =P(X_1=w|W=0)P_0(W=0)= 0 \cdot \frac{1}{4} = 0 \\P(W=1, X_1=w) & =P(X_1=w|W=1)P_0(W=1)= \frac{1}{3} \cdot \frac{1}{4} = \frac{1}{12} \\P(W=2, X_1=w) & =P(X_1=w|W=2)P_0(W=2)= \frac{2}{3} \cdot \frac{1}{4} = \frac{1}{6} \\P(W=3, X_1=w) & =P(X_1=w|W=3)P_0(W=3)= 1 \cdot \frac{1}{4} = \frac{1}{4} \\\end{cases}{{</katex>}}

また(1)式の分母である周辺確率は
{{<katex display>}}P(X_1=w)=\sum_{W}P(W, X_1=w) = 0+ \frac{1}{12} + \frac{1}{6} + \frac{1}{4} = \frac{1}{2}{{</katex>}}
となり、同様に{{<katex>}}P(X_1=w)=\frac{1}{2}{{</katex>}}となる。

(1)式にこれらの結果を代入すると
{{<katex display>}}\begin{cases}P(W=0|X_1=w) & = 0 / \frac{1}{2} = 0\\ P(W=1|X_1=w) & = \frac{1}{12} / \frac{1}{2} =\frac{1}{6}\\ P(W=2|X_1=w) & = \frac{1}{6} / \frac{1}{2} =\frac{1}{3}\\ P(W=3|X_1=w) & = \frac{1}{4} / \frac{1}{2} =\frac{1}{2}\\ \end{cases}{{</katex>}}
となり、１回目に白ボールが出た場合、確率的には箱の中のボールは全部白の可能性が一番高いと推論できることを示している。

### 【ケースB】３回の試行で「白→赤→白」が出た場合
ケースAと同様の考察を繰り返すと良い。
要点は、ボールを試行の度に箱に戻すため、Wが決定された状態では、各試行間は独立なので
{{<katex display>}}P(X_1,X_2,X_3)=P(X_1)P(X_2)P(X_3){{</katex>}}
となることを利用する。ここで実際の計算は練習問題としておく。
